<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="AnyTouch2">
    <meta name="keywords" content="Multi-Sensory, Robotic Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>AnyTouch2</title>

    <!-- <link rel="icon" href="asset/moka.png"> -->

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        packages: { '[+]': ['ams'] }
      },
      svg: {
        scale: 1, // 不缩放
        fontCache: 'global'
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <style>
    body {
      font-family: sans-serif;
      padding: 1rem;
    }

    .math-scroll {
      overflow-x: auto;
      max-width: 100%;
      padding: 0.5rem 0;
    }

    mjx-container {
      display: inline-block;
      min-width: max-content; /* 确保公式可滚动 */
    }
  </style>

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true } });
    </script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/logo.png"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .card {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 10px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 7px;
            background-color: #f1f1f1;
        }

        .card:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }

        .card2 {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 7px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 5px;
            background-color: #f1f1f1;
        }

        .card2:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }
    </style>
</head>

<body>
    <style>
        .img-box {
        position: relative;
        display: inline-block;   /* 宽高由内容决定 */
        }
        .img {
            position: absolute;
            object-fit: cover;
            opacity: 0;
            transition: opacity 0.8s ease;
            cursor: pointer;
        }
        .img.base {
        position: relative;      /* 保留在文档流 */
        opacity: 1;              /* 一直可见或受 active 控制 */
        }

        .img.overlay {
        position: absolute;
        top: 0;
        left: 0;
        }

        .img.active {
            opacity: 1;
            
        }

    </style>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://gewu-lab.github.io/">
                            GeWu Lab@RUC
                        </a>

                    </div>
                </div>
            </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-2 publication-title">AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception</h2>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Ruoxuan Feng<sup>1,2,3</sup>,</span>
                            <span class="author-block">
                                Yuxuan Zhou<sup>4</sup>,</span>
                            <span class="author-block">
                                Siyu Mei<sup>4</sup>,</span>
                            <span class="author-block">
                                Dongzhan Zhou<sup>5</sup>,</span>
                            <span class="author-block">
                                Pengwei Wang<sup>3</sup>,</span> 
                            <span class="author-block">
                                Shaowei Cui<sup>6,3</sup>,</span>   
                            <span class="author-block">
                                Bin Fang<sup>7,3</sup>,</span> 
                            <span class="author-block">
                                Guocai Yao<sup>3,8</sup>,</span>    
                            <span class="author-block">
                                Di Hu<sup>1,2,3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Gaoling School of Artificial Intelligence Renmin
                                University of China Beijing, China
                                <span class="author-block"><sup>2</sup>
                                    Beijing Key Laboratory of Research on Large Models and Intelligent Governance</span>
                                <span class="author-block"><sup>3</sup>
                                    Beijing Academy of Artificial Intelligence</span>
                                <span class="author-block"><sup>4</sup>
                                    Beijing Jiaotong University</span>
                                <span class="author-block"><sup>5</sup>
                                    Shanghai Artificial Intelligence Laboratory</span>
                                <span class="author-block"><sup>6</sup>
                                    Institute of Automation, Chinese Academy of Sciences</span>
                                <span class="author-block"><sup>7</sup>
                                    Beijing University of Posts and Telecommunications</span>
                                <span class="author-block"><sup>8</sup>
                                    State Key Laboratory of Multimedia Information Processing, Peking University</span>    
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/GeWu-Lab/AnyTouch2"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://huggingface.co/collections/BAAI/touchd"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database main-icon"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href=""
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-dice-d6"></i>
                                        </span>
                                        <span>Checkpoint</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
    </section>


    <section >
        <div class="container is-max-desktop">
            <div class="hero-body" style="padding-top: 12px;">
                <h2 class="title is-3 has-text-centered" style="margin: 4px;">Overview</h2>
                <div class="content has-text-justified">
                    <div class="img-box">
                    <img src='asset/first.png' width="100%" height="100%" valign="center" class="img base active">
                    <img src='asset/second.png' width="100%" height="100%" valign="center" class="img overlay">
                </div>

                    <p>
                        The rise of high-resolution optical tactile sensors are ushering robotics into an era of <b>dynamic tactile perception</b>, 
                        where robots can sense the temporal variations in contact, force, and material interaction for increasingly complex real-world tasks.
                        In stark contrast, existing tactile datasets and models are fundamentally unable to support this revolution.
                        They remain limited to static, object-level properties, leaving the rich temporal dynamics of touch largely unexplored.
                        <b style="color: rgb(197, 90, 17);">Today, we want to bridge this gap.</b> We need an entirely new <b style="color: rgb(197, 90, 17);">dynamic data ecosystem</b> with 
                        corresponding <b style="color: rgb(197, 90, 17);">datasets</b>, along with a <b style="color: rgb(197, 90, 17);">general-purpose model</b> that can 
                        comprehensively cover tactile perception abilities—especially dynamic tactile perception.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <script>
        document.querySelectorAll('.img-box').forEach(box => {
        let used = false;

        box.addEventListener('click', () => {
            if (used) return;
            const imgs = box.querySelectorAll('.img');
            imgs[0].classList.remove('active');
            imgs[1].classList.add('active');
            used = true;
        });
        });
    </script>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Tactile Dynamic Pyramid & ToucHD Dataset</h2>

                <div class="content has-text-justified">
                <p>
                    To establish a systematic paradigm for dynamic tactile perception, introduce a tactile 
                    dynamic pyramid that organizes tactile data into five tiers based on the complexity level 
                    of the perception capabilities they support:
                </p>
                <ul>
                        <li><b>Tier 5 (Press-Only):</b> Collected by <b> only pressing the sensor against
                        objects </b> using either handheld operation or a robot arm. It mainly supports the recognition of 
                        object-level attribute. (Touch and Go, ObjectFolder, VisGel, TVL, etc.)</li>
                        <li><b>Tier 4 (Press-Only):</b> Collected by <b> pressing the sensor against
                        objects, followed by random sliding and rotation </b>. It enables perception of surface-related dynamics 
                        but lacking task relevance. (YCB-Slide, TacQuad, etc.)</li>
                        <li><b>Tier 3 (Specific Action):</b> Collected by <b> controlling
                        the sensor to press and slide along the object surface </b>following specific predefined actions. 
                        It can facilitate action-level tactile understanding.</li>
                        <li><b>Tier 2 (Manipulation Data):</b> Collected during <b> real object manipulation tasks </b>
                        using a robot arm or a UMI device. It is  essential for learning real-world manipulation skills</li>
                        <li><b>Tier 1 (Force Data):</b> Collected by <b> a robot arm equipped with a force
                        sensor.</b> It enables reasoning about force–deformation relationships and supporting fine-grained, 
                        force-sensitive manipulation tasks. (e.g. FeelAnyForce)</li>
                    </ul>
                Most existing tactile datasets reside in Tier 4 and 5, offering insufficient support for advanced dynamic perception tasks such as dexterous manipulation, while <b>higher-tier
                data</b> remain scarce. To address this gap, we present <b>ToucHD</b>, a large-scale tactile dataset with 2,426,174 contact samples
                designed as a Tactile Hierarchical Dynamic resource to enrich higher-tier dynamic tactile data.
                <b style="color: rgb(197, 90, 17);">Compared with existing tactile datasets,
                ToucHD has advantages in terms of scale, sensor diversity, label diversity, and dynamic diversity.</b>
                The dataset comprises three subsets corresponding to the highest 3 tiers of the pyramid:
                <ul>
                    <div class="container is-max-desktop" style="padding-bottom: 12px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" id="pour" autoplay controls muted loop playsinline height="100%" style="margin-bottom: -6.9px;">
                                    <source src="./asset/sim-rotate.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>

                        <div class="column">
                            <div class="card">
                                <video poster="" id="peg" autoplay controls muted loop playsinline style="margin-bottom: -6.9px;">
                                    <source src="./asset/sim-move.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
                        <li><b>Simulated Atomic Action Data (Sim).</b> We collect 1,118,896 multi-sensor contact frames from five optical tactile sensors 
                            performing 6 atomic actions—sliding left/right/up/down and rotating clockwise/counterclockwise on 1,043 3D objects.</li>
                        <!-- <img src="asset/sim.png" alt="Simulated Atomic Action Data" width="100%" height="100%"> -->
                        
                        <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 30px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" id="pour" autoplay controls muted loop playsinline height="100%" style="margin-bottom: -6.9px;">
                                    <source src="./asset/umi-1.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>

                        <div class="column">
                            <div class="card">
                                <video poster="" id="peg" autoplay controls muted loop playsinline style="margin-bottom: -6.9px;">
                                    <source src="./asset/umi-2.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
                        
                        <li><b>Real-World Manipulation Data (Mani).</b> We modify FastUMI by equipping its two grippers with different tactile sensors and 
                        collect 584,842 contact frames from 46 carefully designed manipulation tasks, while simultaneously recording the interaction videos.</li>
                        <!-- <img src="asset/sim.png" alt="Simulated Atomic Action Data" width="100%" height="100%"> -->
                         
                        <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 30px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" id="pour" autoplay controls muted loop playsinline height="100%" style="margin-bottom: -6.9px;">
                                    <source src="./asset/force-1.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>

                        <div class="column">
                            <div class="card">
                                <video poster="" id="peg" autoplay controls muted loop playsinline style="margin-bottom: -6.9px;">
                                    <source src="./asset/force-2.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
                        
                        <li><b>Touch-Force Paired Data (Force).</b> We collect 722,436 touch–force pairs using five carefully selected tactile sensors and 71 distinct indenters.
                        Under programmatic control, each indenter performs sliding motions in four directions—forward, backward, left, and right—across the sensor
                        surface, while a wrist-mounted force sensor records 3D contact force sequences. </li>
                    </ul>


            </div>
        </div>


    </section>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">AnyTouch 2 Model</h2>

                <div class="content has-text-justified">
                <img src="asset/model.png" alt="AnyTouch 2 Model" width="80%" height="80%" style="display: block; margin: auto; padding-bottom: 18px;">
                <p>
                    Building on this dynamic tactile data ecosystem, we introduce <b>AnyTouch 2</b>, a general tactile representation learning framework with <b>comprehensive multi-level dynamic perception capabilities</b>:
                </p>
                <ul>
                <li>
                    <p>
                    <b>Pixel-Level Dynamic Details.</b> We employ video masked autoencoder to reconstruct the masked video frames and the frame differences, enabling the model to capture fine-grained temporal variations essential for dynamic perception.
                    </p>
                </li>
                <li>
                    <p>
                    <b>Semantic-Level Tactile Features.</b> We employ multi-modal alignment, object matching and cross-sensor matching to capture both static objectlevel and dynamic action-aware semantic features, effectively bridging low-level tactile signals with
high-level perceptual understanding.
                    </p>
                </li>
                <li>
                    <p>
                    <b>Dynamic Physical Properties.</b> We introduce the force and delta force prediction task to explicitly model the physical
properties underlying tactile interactions based on ToucHD. This design enables a comprehensive, physically grounded representation spanning all tiers of the tactile dynamic
pyramid, supports dexterous manipulation, and <b>allows the model to stand out from prior methods</b>.
                    </p>
                </li>
                </ul>
            </div>
        </div>

        
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Experiments</h2>
                <h2 class="title is-4">Online Real-World Manipulation</h2>

                <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 6px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/grasp_success_demo.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">Tactile Grasping (Tier 5)</p>
                            </div>
                        </div>
                </div>
                </div>
                <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 6px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/wipe_success_demo.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">Whiteboard Wiping (Tier 4 & 3)</p>
                            </div>
                        </div>
                </div>
                </div>
                <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 6px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/usb_success_demo.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">USB Insertion (Tier 2)</p>
                            </div>
                        </div>
                </div>
                </div>
                <div class="container is-max-desktop" style="padding-bottom: 12px; padding-top: 6px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/chip_success_demo.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">Chip Moving (Tier 1)</p>
                            </div>
                        </div>
                </div>
                </div>
                <div class="content has-text-justified">
                
                <p>
                    We design four challenging real-world manipulation
                    tasks that explicitly span the tactile dynamic pyramid: Tactile Grasping (Tier 5), Whiteboard Wiping (Tier 4 & 3), 
                    USB Insertion (Tier 2) and Chip Moving (Tier 1). These tasks
comprehensively cover all tiers of the dynamic pyramid, from object-level property recognition TO force-sensitive precision manipulation.
                </p>

                <img src="asset/onlineresult.png" alt="Online" width="80%" height="80%" style="display: block; margin: auto; padding-bottom: 18px;">
                <p>
                    AnyTouch 2 achieves the strongest Tier-1 dynamic perception capability, outperforming all baselines across all 4 real-world tasks.
                    <b>This also marks a successful effort to incorporate UMI device with tactile sensors into model training.</b>
                </p>
                
            </div> 
            <h2 class="title is-4">Sensor Generalization</h2>
            <div class="container is-max-desktop" style="padding-bottom: 18px; padding-top: 6px;">
                    <div class="columns is-centered">
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/grasp_old_fixed.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">Tactile Grasping (DIGIT)</p>
                            </div>
                        </div>
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/wipe_old_fixed.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">Whiteboard Wiping (DIGIT)</p>
                            </div>
                        </div>
                        <div class="column">
                            <div class="card">
                                <video poster="" autoplay controls muted loop playsinline height="100%" style="margin-bottom: 0px;">
                                    <source src="./asset/usb_old_fixed.mp4" type="video/mp4">
                                </video>
                                <p style="text-align: center; font-weight: bold;">USB Insertion (DIGIT)</p>
                            </div>
                        </div>
                </div>
                </div>
                <div class="content has-text-justified">
                <p>
                    AnyTouch 2 integrates multiple optical tactile sensors, such as GelSight, GelSight Mini, DIGIT, GelSlim, and Duragel, and exhibits sensor generalization capability across various downstream tasks.
                </p>
                
            </div>

                <h2 class="title is-4">Offline Benchmark Evaluation</h2>
                <div class="content has-text-justified">
                <img src="asset/result.png" alt="Offline" width="80%" height="80%" style="display: block; margin: auto; padding-bottom: 18px;">
                <p>
                    AnyTouch 2 demonstrates superior performance across all tactile perception tasks, covering both object-level static attribute understanding
                    and physical-level dynamic perception.
                </p>
                
            </div>
 
        </div>

        
    </section>


<div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>Coming Soon</code></pre>
    </div>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://openreview.net/pdf?id=ndilONnABZ">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/GeWu-Lab/AnyTouch2" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            Thanks to <a href="https://nerfies.github.io/">Nerfies</a> for providing the
                            template of this page.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>